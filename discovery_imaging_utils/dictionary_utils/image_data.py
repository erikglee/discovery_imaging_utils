import numpy as np
import os
import nibabel as nib
from discovery_imaging_utils import nifti_utils
from discovery_imaging_utils import gifti_utils
import json
import h5py



def populate(lh_gii_data_path=None,
			lh_inclusion_mask_path=None,
			lh_parcellation_path=None,
			rh_gii_data_path=None,
			rh_inclusion_mask_path=None,
			rh_parcellation_path=None,
			nifti_data_path=None,
			nifti_inclusion_mask_path=None,
			nifti_parcellation_path=None,
			normalize = True):
	"""Takes a file_path_dictionary and uses it to populate an image_data_dictionary

	Takes a file_path_dictionary generated by, generate_file_paths, and creates an
	image_data_dictionary.


	Parameters
	----------

	file_path_dictionary : dict
		dictionary created by generate_file_paths. Must have at a minimum entries
		with paths for

	TR : float
		the repitition time in seconds

	normalize : bool
		whether data should be normalized (defaults to true). Automatically
		done prior to parcellation, but this determines if the output data
		will be mean of 10k. Only works for 4d nifti or 2d surface data.


	Returns
	-------

	image_data_dictionary : dict
	.....input details



	"""


	file_path_dictionary = {}

	if type(lh_gii_data_path) != type(None):
		file_path_dictionary['lh_gii_data_path'] = lh_gii_data_path
	if type(lh_inclusion_mask_path) != type(None):
		file_path_dictionary['lh_inclusion_mask_path'] = lh_inclusion_mask_path
	if type(lh_parcellation_path) != type(None):
		file_path_dictionary['lh_parcellation_path'] = lh_parcellation_path
	if type(rh_gii_data_path) != type(None):
		file_path_dictionary['rh_gii_data_path'] = rh_gii_data_path
	if type(rh_inclusion_mask_path) != type(None):
		file_path_dictionary['rh_inclusion_mask_path'] = rh_inclusion_mask_path
	if type(rh_parcellation_path) != type(None):
		file_path_dictionary['rh_parcellation_path'] = rh_parcellation_path
	if type(nifti_data_path) != type(None):
		file_path_dictionary['nifti_data_path'] = nifti_data_path
	if type(nifti_inclusion_mask_path) != type(None):
		file_path_dictionary['nifti_inclusion_mask_path'] = nifti_inclusion_mask_path
	if type(nifti_parcellation_path) != type(None):
		file_path_dictionary['nifti_parcellation_path'] = nifti_parcellation_path



	image_data_dict = {}
	metadata_dict = {}
	metadata_dict['filepaths_dict'] = file_path_dictionary

	lh_data = None
	rh_data = None
	nifti_data = None

	has_lh_gifti = False
	has_rh_gifti = False
	has_lh_gifti_parcellation = False
	has_rh_gifti_parcellation = False
	has_nifti = False
	has_nifti_parcellation = False

	#If there is lh surface data
	if 'lh_gii_data_path' in file_path_dictionary.keys():

		has_lh_gifti = True
		lh_data = gifti_utils.load_gifti_func(file_path_dictionary['lh_gii_data_path'])
		image_data_dict['lh_gifti_shape'] = lh_data.shape
		lh_gifti_ids = np.arange(0, lh_data.shape[0], 1, dtype=int)


		#If inclusion mask is specified, set zero values in inclusion
		#mask to NaN.. the _parcellate_gifti function knows how to handle this
		if 'lh_inclusion_mask_path' in file_path_dictionary.keys():

			#Make function to set appropriate values to NaN.....
			lh_data, lh_inclusion_inds = gifti_utils.incorporate_gifti_inclusion_mask(lh_data, file_path_dictionary['lh_inclusion_mask_path'])
			lh_gifti_ids = lh_inclusion_inds



		if 'lh_parcellation_path' in file_path_dictionary.keys():

			has_lh_gifti_parcellation = True
			#Need to (1) parcellate data, (2) return parcel labels, (3) save info to recreate parcels
			lh_data, lh_labels, lh_parcels_dict = gifti_utils.parcellate_gifti(lh_data, file_path_dictionary['lh_parcellation_path'])
			lh_gifti_ids = lh_labels
			image_data_dict['lh_parcels_dict'] = lh_parcels_dict


	#If there is rh surface data
	if 'rh_gii_data_path' in file_path_dictionary.keys():

		has_rh_gifti = True
		rh_data = gifti_utils.load_gifti_func(file_path_dictionary['rh_gii_data_path'])
		image_data_dict['rh_gifti_shape'] = rh_data.shape
		rh_gifti_ids = np.arange(0, rh_data.shape[0], 1, dtype=int)


		#If inclusion mask is specified, set zero values in inclusion
		#mask to NaN.. the _parcellate_gifti function knows how to handle this
		if 'rh_inclusion_mask_path' in file_path_dictionary.keys():

			#Make function to set appropriate values to NaN.....
			rh_data, rh_inclusion_inds = gifti_utils.incorporate_gifti_inclusion_mask(rh_data, file_path_dictionary['rh_inclusion_mask_path'])
			rh_gifti_ids = rh_inclusion_inds



		if 'rh_parcellation_path' in file_path_dictionary.keys():

			has_rh_gifti_parcellation = True
			#Need to (1) parcellate data, (2) return parcel labels, (3) save info to recreate parcels
			rh_data, rh_labels, rh_parcels_dict = gifti_utils.parcellate_gifti(rh_data, file_path_dictionary['rh_parcellation_path'])
			rh_gifti_ids = rh_labels
			image_data_dict['rh_parcels_dict'] = rh_parcels_dict



	#If there is nifti data
	if 'nifti_data_path' in file_path_dictionary.keys():

		has_nifti = True
		nifti_img = nib.load(file_path_dictionary['nifti_data_path'])
		nifti_data = nifti_img.get_fdata()
		#Or instead nifti_data = nifti_img.dataobj
		if nifti_data.ndim > 3:
			nifti_3d = np.squeeze(nifti_data[:,:,:,0])
			nifti_ids = np.where(nifti_3d != None)
		else:
			nifti_ids = np.where(nifti_data != None)

		nifti_inclusion_inds = None
		image_data_dict['nifti_affine'] = nifti_img.affine
		image_data_dict['nifti_shape'] = nifti_data.shape

		#remove from memory after done using...
		del nifti_img


		if 'nifti_inclusion_mask_path' in file_path_dictionary.keys():

			nifti_data, nifti_inclusion_inds = nifti_utils.incorporate_nifti_inclusion_mask(nifti_data, file_path_dictionary['nifti_inclusion_mask_path'])
			nifti_ids = nifti_inclusion_inds

		if 'nifti_parcellation_path' in file_path_dictionary.keys():

			has_nifti_parcellation = True
			nifti_data, nifti_labels, nifti_parcels_dict = nifti_utils.parcellate_nifti(nifti_data, file_path_dictionary['nifti_parcellation_path'])
			nifti_ids = nifti_labels

			image_data_dict['nifti_parcels_dict'] = nifti_parcels_dict

		#If the data hasn't already been brought down to 2d, then do that now
		if nifti_data.ndim > 2:

			#Check what the final dimension should be
			if nifti_data.ndim == 3:
				depth = 1
			else:
				depth = nifti_data.shape[3]


			if type(nifti_inclusion_inds) == type(None):

				nifti_data = np.reshape(nifti_data, (nifti_data.shape[0]*nifti_data.shape[1]*nifti_data.shape[2], depth))

			else:

				nifti_data = np.reshape(nifti_data[nifti_inclusion_inds], (nifti_inclusion_inds[0].shape[0], depth))



	#FYI...
	#data inds will specify how to access lh/rh/nifti elements from the data
	#key, and the ids will alternatively specify what those inds mean in terms
	#of the lh/rh/nifti or parcellation schemes

	if (has_lh_gifti or has_rh_gifti or has_nifti) == False:
		raise NameError('Error: Must define path for at least one of the following - lh_gifti_data, rh_gifti_data, nifti_data')

	num_dimensions = []
	if type(lh_data) != type(None):
		num_dimensions.append(lh_data.shape[1])
	if type(rh_data) != type(None):
		num_dimensions.append(rh_data.shape[1])
	if type(nifti_data) != type(None):
		num_dimensions.append(nifti_data.shape[1])

	if np.unique(num_dimensions).shape[0] != 1:
		raise NameError('Error: LH, RH, and Nifti data must all have the same length.')

	data = None

	#Add lh gifti data
	if type(lh_data) != type(None):
		image_data_dict['lh_data_inds'] = np.arange(0, len(lh_gifti_ids), 1, dtype=int)
		data = lh_data

		image_data_dict['lh_ids'] = lh_gifti_ids


	#Add rh gifti data
	if type(rh_data) != type(None):
		if type(data) != type(None):
			rh_data_inds = np.arange(data.shape[0], data.shape[0] + len(rh_gifti_ids), 1, dtype=int)
			data = np.vstack((data, rh_data))
		else:
			rh_data_inds = np.arange(0, rh_data.shape[0], 1, dtype=int)
			data = rh_data

		image_data_dict['rh_data_inds'] = rh_data_inds
		image_data_dict['rh_ids'] = rh_gifti_ids




	#Add nifti data
	if type(nifti_data) != type(None):
		if type(data) != type(None):
			nifti_data_inds = np.arange(data.shape[0], data.shape[0] + nifti_data.shape[0], 1, dtype=int)
			data = np.vstack((data, nifti_data))
		else:
			nifti_data_inds = np.arange(0, nifti_data.shape[0], 1, dtype=int)
			data = nifti_data

		image_data_dict['nifti_data_inds'] = nifti_data_inds
		#image_data_dict['nifti_ids'] = ['nii_' + str(temp_label) for temp_label in nifti_data_inds]
		image_data_dict['nifti_ids'] = nifti_ids


	#Normaize data if necessary
	if normalize == True:
		if data.shape[1] > 1:

			image_data_dict['data_means'] = np.mean(data,axis=1)
			data = data/image_data_dict['data_means'][:,np.newaxis]*10000



	image_data_dict['data'] = data


	return image_data_dict




def convert_to_images(image_data_dict, output_folder, overwrite = False):

	"""Function that converts image data dictionaries back to nifti/gifti

	Takes an image data dictionary, possibly parcellated and or transformed
	data from a gifti and or nifti file and uses information about the base
	files (affine, size, parcel_ids) from the image_data_dict and saves
	the corresponding gifti/nifti files in a new folder
	data, and

	Parameters
	----------

	image_data_dict : dict
	dictionary whose data will be used to reconstruct nifti and
	gifti files when relevant

	output_folder : str
	path to folder that will be created to store the nifti/gifti files

	overwrite : bool
	whether to continue if the output_folder already exists

	"""

	if os.path.exists(output_folder):

		if overwrite == False:

			raise NameError('Error: folder already exists')

	else:

		os.makedirs(output_folder)


	#Make LH gifti if necessary
	if 'lh_data_inds' in image_data_dict.keys():

		lh_data = image_data_dict['data'][image_data_dict['lh_data_inds']]
		lh_gifti_data = np.zeros(image_data_dict['lh_gifti_shape'])


		#Unparcellate the data (this only works because dictionaries
		#are now ordered in python....)
		if 'lh_parcels_dict' in image_data_dict.keys():

			i = 0
			for parcel, inds in image_data_dict['lh_parcels_dict'].items():
				lh_gifti_data[inds] = lh_data[i]
				i += 1

		else:

			lh_gifti_data[image_data_dict['lh_ids']] = lh_data

		lh_gifti_path = os.path.join(output_folder, 'lh.data.func.gii')
		gifti_utils.arr2gifti(lh_gifti_data, lh_gifti_path)


	#Make RH gifti if necessary
	if 'rh_data_inds' in image_data_dict.keys():

		rh_data = image_data_dict['data'][image_data_dict['rh_data_inds']]
		rh_gifti_data = np.zeros(image_data_dict['rh_gifti_shape'])

		#Unparcellate the data (this only works because dictionaries
		#are now ordered in python....)
		if 'rh_parcels_dict' in image_data_dict.keys():

			i = 0
			for parcel, inds in image_data_dict['rh_parcels_dict'].items():
				rh_gifti_data[inds] = rh_data[i]
				i += 1

		else:

			rh_gifti_data[image_data_dict['rh_ids']] = rh_data

		rh_gifti_path = os.path.join(output_folder, 'rh.data.func.gii')
		gifti_utils.arr2gifti(rh_gifti_data, rh_gifti_path)


	if 'nifti_data_inds' in image_data_dict.keys():

		nifti_partial_data = image_data_dict['data'][image_data_dict['nifti_data_inds']]
		nifti_data = np.zeros(image_data_dict['nifti_shape'])

		#Unparcellate the data
		if 'nifti_parcels_dict' in image_data_dict.keys():

			i = 0
			for parcel, inds in image_data_dict['nifti_parcels_dict'].items():
				nifti_data[inds] = nifti_partial_data[i]
				i += 1

		else:

			if nifti_partial_data.shape[1] == 1:
				nifti_data[image_data_dict['nifti_ids']] = np.squeeze(nifti_partial_data)
			else:
				x = image_data_dict['nifti_ids'][0]
				y = image_data_dict['nifti_ids'][1]
				z = image_data_dict['nifti_ids'][2]
				nifti_data[x,y,z,:] = nifti_partial_data

		nifti_path = os.path.join(output_folder, 'data.nii.gz')
		nifti_utils.arr2nifti(nifti_data, image_data_dict['nifti_affine'], nifti_path)

	return


def populate_hdf5(hdf5_file_path,
			lh_gii_data_path=None,
			lh_inclusion_mask_path=None,
			lh_parcellation_path=None,
			rh_gii_data_path=None,
			rh_inclusion_mask_path=None,
			rh_parcellation_path=None,
			nifti_data_path=None,
			nifti_inclusion_mask_path=None,
			nifti_parcellation_path=None,
			normalize_within_parcels = False,
			normalize_within_dataset = True,
			overwrite = False):
	"""Function that creates an HDF5 file to store various neuroimaging datas

	The purpose of this function is to create an hdf5 file referred to as an
	'image_data_dictionary' that can be used to facilitate easy use of neuroimaging
	data for different analyses. With details outlined below, this structure
	supports scalar (i.e. anatomical), or vector (i.e. functional) data, in
	both volumetric and surface spaces, allowing for sparse representation (i.e.
	excluding a brain mask), and mixed parcellation schemes (i.e. you could
	parcellate one hemisphere but not the other if desired). The main element
	of the created HDF5 file is its dataset 'data' that contains combined data
	from all input sources. An important feature of this data structure, is that
	all information required to convert the data back into original form is
	contained within the HDF5, such that after future data manipulations, the
	resulting data structure can easily be converted back to nifti/gifti files
	so that the results can be viewed with standard neuroimaging visualization
	packages.

	This function can take 1-3 sources, constituting nifti, lh gifti, and rh
	gifti data. In simplest form, if paths to all three possible source types
	are given, then this function will create a hdf5 file with a 'data' dataset
	that contains the combined data from all sources, with shape
	<n_regions, n_timepoints> (scalar, and vector data sources are supported)

	'data' will be a virtual dataset that allows you to access datasets for
	'lh_data', 'rh_data', and 'nifti_data' together, alternatively you can
	access data from an individual source by either looking at their unique
	datasets (i.e. lh_data) or by using 'lh_ids', 'rh_ids', or 'nifti_ids',
	which tell you which elements in 'data' belong to the different data sources.

	Each source will also have its own data_inds dataset such as 'lh_data_inds'.
	The data_inds datasets show what each element of 'data' is in the original
	source format (i.e. which voxel, vertex, or parcel depending on usage). In
	the most basic case this will give one index per location in the data source.
	If a inclusion mask is incorporated, this will only index desired voxels/
	vertices. If a parcellation is included, this will represent the names of
	the different parcels/ROIs.

	Metadata regarding input file information (dimensions, names, etc.) can be
	found as attributes under the 'data' dataset.


	Parameters
	----------

	hdf5_file_path : dict
		dictionary created by generate_file_paths. Must have at a minimum entries
		with paths for

	TR : float
		the repitition time in seconds

	normalize_within_parcels : bool, defaults to False
		if true, and if parcels are provided, and if data has temporal dimension,
		then voxels/vertices contributing to parcels/rois will be temporally
		demeaned before averaging, AND then the average voxel/vertex intensity
		will be inserted back into the time course.
	normalize_within_dataset : bool, defaults to True
		whether data should be normalized to have a global mean of 1k. Currently
		only supported for data with temporal dimension.



	"""

	#HOW TO SET ATTRIBUTES REMINDER:
	#dataset.attrs['name_of_desired_attribute'] = attribute

	if overwrite == True:
		if os.path.exists(hdf5_file_path):
			os.remove(hdf5_file_path)

	with h5py.File(hdf5_file_path, 'w') as f:


		file_path_dictionary = {}
		metadata_dict = {}
		image_data_dict = {}

		if type(lh_gii_data_path) != type(None):
			print('Found LH Gifti Input')
			file_path_dictionary['lh_gii_data_path'] = lh_gii_data_path
			if type(lh_inclusion_mask_path) != type(None):
				file_path_dictionary['lh_inclusion_mask_path'] = lh_inclusion_mask_path
			if type(lh_parcellation_path) != type(None):
				file_path_dictionary['lh_parcellation_path'] = lh_parcellation_path
		if type(rh_gii_data_path) != type(None):
			print('Found RH Gifti Input')
			file_path_dictionary['rh_gii_data_path'] = rh_gii_data_path
			if type(rh_inclusion_mask_path) != type(None):
				file_path_dictionary['rh_inclusion_mask_path'] = rh_inclusion_mask_path
			if type(rh_parcellation_path) != type(None):
				file_path_dictionary['rh_parcellation_path'] = rh_parcellation_path
		if type(nifti_data_path) != type(None):
			print('Found Nifti Input')
			file_path_dictionary['nifti_data_path'] = nifti_data_path
			if type(nifti_inclusion_mask_path) != type(None):
				file_path_dictionary['nifti_inclusion_mask_path'] = nifti_inclusion_mask_path
			if type(nifti_parcellation_path) != type(None):
				file_path_dictionary['nifti_parcellation_path'] = nifti_parcellation_path



		#metadata_dict['filepaths_dict'] = file_path_dictionary

		lh_data = None
		rh_data = None
		nifti_data = None

		has_lh_gifti = False
		has_rh_gifti = False
		has_lh_gifti_parcellation = False
		has_rh_gifti_parcellation = False
		has_nifti = False
		has_nifti_parcellation = False

		#If there is lh surface data
		if 'lh_gii_data_path' in file_path_dictionary.keys():

			#UPDATE THESE ITEMS WITH READ_DIRECT FUNCTION TO
			#LOWER MEMORY USAGE!!!!!

			has_lh_gifti = True
			f['lh_data'] = gifti_utils.load_gifti_func(lh_gii_data_path)
			metadata_dict['lh_gifti_shape'] = f['lh_data'].shape #or could put this as an attribute?
			lh_gifti_ids = np.arange(0, f['lh_data'].shape[0], 1, dtype=int)


			#If inclusion mask is specified, set zero values in inclusion
			#mask to NaN.. the _parcellate_gifti function knows how to handle this
			if 'lh_inclusion_mask_path' in file_path_dictionary.keys():

				#Make function to set appropriate values to NaN.....
				f['lh_data_masked'], lh_inclusion_inds = gifti_utils.incorporate_gifti_inclusion_mask(f['lh_data'], lh_inclusion_mask_path)
				lh_gifti_ids = lh_inclusion_inds

				del f['lh_data']
				f.create_dataset('lh_data', data = f['lh_data_masked'], compression = 'gzip')
				del f['lh_data_masked']



			if 'lh_parcellation_path' in file_path_dictionary.keys():

				has_lh_gifti_parcellation = True
				#Need to (1) parcellate data, (2) return parcel labels, (3) save info to recreate parcels
				f['lh_data_masked'], lh_labels, lh_parcels_dict = gifti_utils.parcellate_gifti(f['lh_data'], lh_parcellation_path, intensity_normalize_before_averaging = normalize_within_parcels)
				lh_gifti_ids = lh_labels
				#image_data_dict['lh_parcels_dict'] = lh_parcels_dict

				del f['lh_data']
				f.create_dataset('lh_data', data = f['lh_data_masked'], compression = 'gzip')
				del f['lh_data_masked']

			if has_lh_gifti_parcellation:
				_dict_to_hdf5_subdatasets(f, lh_parcels_dict, '/parcels/lh')

			print('Finished Loading LH Data')


		#If there is rh surface data
		if 'rh_gii_data_path' in file_path_dictionary.keys():

			has_rh_gifti = True
			f['rh_data'] = gifti_utils.load_gifti_func(rh_gii_data_path)
			metadata_dict['rh_gifti_shape'] = f['rh_data'].shape
			rh_gifti_ids = np.arange(0, f['rh_data'].shape[0], 1, dtype=int)


			#If inclusion mask is specified, set zero values in inclusion
			#mask to NaN.. the _parcellate_gifti function knows how to handle this
			if 'rh_inclusion_mask_path' in file_path_dictionary.keys():

				#Make function to set appropriate values to NaN.....
				f['rh_data_masked'], rh_inclusion_inds = gifti_utils.incorporate_gifti_inclusion_mask(f['rh_data'], rh_inclusion_mask_path)
				rh_gifti_ids = rh_inclusion_inds

				del f['rh_data']
				f.create_dataset('rh_data', data = f['rh_data_masked'], compression = 'gzip')
				del f['rh_data_masked']



			if 'rh_parcellation_path' in file_path_dictionary.keys():

				has_rh_gifti_parcellation = True
				#Need to (1) parcellate data, (2) return parcel labels, (3) save info to recreate parcels
				f['rh_data_masked'], rh_labels, rh_parcels_dict = gifti_utils.parcellate_gifti(f['rh_data'], rh_parcellation_path, intensity_normalize_before_averaging = normalize_within_parcels)
				rh_gifti_ids = rh_labels
				#image_data_dict['rh_parcels_dict'] = rh_parcels_dict

				del f['rh_data']
				f.create_dataset('rh_data', data = f['rh_data_masked'], compression = 'gzip')
				del f['rh_data_masked']


			if has_rh_gifti_parcellation:
				_dict_to_hdf5_subdatasets(f, rh_parcels_dict, '/parcels/rh')
			print('Finished Loading RH Data')

		#If there is nifti data
		if 'nifti_data_path' in file_path_dictionary.keys():

			#STILL NEED TO ADD COMPRESSION FOR IF PARCELLATION
			#ISNT USED (OR BRAIN MASK) (ALSO FOR HEMIS)

			has_nifti = True
			nifti_img = nib.load(nifti_data_path)
			nifti_data = f.create_dataset("nifti_data", nifti_img.dataobj.shape)
			nifti_data[...] = nifti_img.dataobj[...] #Could also do this through dataobj but would be slower
			metadata_dict['nifti_affine'] = nifti_img.affine
			metadata_dict['nifti_shape'] = f['nifti_data'].shape

			#Find indices to map back to nifti image
			nifti_3d = np.zeros(metadata_dict['nifti_shape'][0:3])
			nifti_ids = np.where(nifti_3d != None)

			nifti_inclusion_inds = None

			#remove from memory after done using...
			del nifti_img
			del nifti_3d


			if 'nifti_inclusion_mask_path' in file_path_dictionary.keys():

				f['nifti_data_masked'], nifti_inclusion_inds = nifti_utils.incorporate_nifti_inclusion_mask(f['nifti_data'], nifti_inclusion_mask_path)
				nifti_ids = nifti_inclusion_inds

				del f['nifti_data']
				f.create_dataset('nifti_data', data = f['nifti_data_masked'], compression = 'gzip')
				del f['nifti_data_masked']


			if 'nifti_parcellation_path' in file_path_dictionary.keys():

				has_nifti_parcellation = True
				f['nifti_data_masked'], nifti_labels, nifti_parcels_dict = nifti_utils.parcellate_nifti(f['nifti_data'], nifti_parcellation_path, demedian_before_averaging = normalize_within_parcels)
				nifti_ids = nifti_labels

				del f['nifti_data']
				f.create_dataset('nifti_data', data = f['nifti_data_masked'], compression = 'gzip')
				del f['nifti_data_masked']

			if has_nifti_parcellation:
				_dict_to_hdf5_subdatasets(f, nifti_parcels_dict, '/parcels/lh')
			print('Finished Loading Nifti Data')



			#MOST TIME DEMAND HAPPENS HERE ONWARD!!!!

			#If the data hasn't already been brought down to 2d, then do that now
			if f['nifti_data'].ndim > 2:

				#Check what the final dimension should be
				if f['nifti_data'].ndim == 3:
					depth = 1
				else:
					depth = f['nifti_data'].shape[3]


					#FIX BELOW FOR HDF5!!!!!!!!
				if type(nifti_inclusion_inds) == type(None):

					reshaped_nifti_data = np.reshape(f['nifti_data'], (f['nifti_data'].shape[0]*f['nifti_data'].shape[1]*f['nifti_data'].shape[2], depth))

				else:

					reshaped_nifti_data = np.reshape(f['nifti_data'][nifti_inclusion_inds], (nifti_inclusion_inds[0].shape[0], depth))

				del f['nifti_data']
				f['nifti_data'] = reshaped_nifti_data

			print('Finished Reshaping Nifti Data')



		#FYI...
		#data inds will specify how to access lh/rh/nifti elements from the data
		#key, and the ids will alternatively specify what those inds mean in terms
		#of the lh/rh/nifti or parcellation schemes

		if (has_lh_gifti or has_rh_gifti or has_nifti) == False:
			raise NameError('Error: Must define path for at least one of the following - lh_gifti_data, rh_gifti_data, nifti_data')

		num_dimensions = []
		num_locations = 0
		if has_lh_gifti:
			num_dimensions.append(f['lh_data'].shape[1])
			num_locations += f['lh_data'].shape[0]
		if has_rh_gifti:
			num_dimensions.append(f['rh_data'].shape[1])
			num_locations += f['rh_data'].shape[0]
		if has_nifti:
			num_dimensions.append(f['nifti_data'].shape[1])
			num_locations += f['nifti_data'].shape[0]

		if np.unique(num_dimensions).shape[0] != 1:
			print(f['lh_data'].shape)
			raise NameError('Error: LH, RH, and Nifti data must all have the same length.')


		hdf5_layout = h5py.VirtualLayout(shape=(int(num_locations), int(num_dimensions[0])))

		#Add lh gifti data
		inds_counted = 0
		if has_lh_gifti:
			f['lh_data_inds'] = np.arange(0, len(lh_gifti_ids), 1, dtype=int)
			#data[inds_counted:(inds_counted + f['lh_data'].shape[0]),:] = f['lh_data']
			inds_counted = int(inds_counted + f['lh_data'].shape[0])

			#Update virtual source/layout
			lh_vsource = h5py.VirtualSource(hdf5_file_path, 'lh_data', shape=f['lh_data'].shape)
			hdf5_layout[0:inds_counted,:] = lh_vsource

			print('Added LH data to HDF5')


		#Add rh gifti data
		if has_rh_gifti:
			if inds_counted > 0:
				image_data_dict['rh_data_inds'] = np.arange(inds_counted, inds_counted + len(rh_gifti_ids), 1, dtype=int)
			else:
				image_data_dict['rh_data_inds'] = np.arange(0, f['rh_data'].shape[0], 1, dtype=int)

			inds_counted = int(inds_counted + f['rh_data'].shape[0])

			#Update virtual source/layout
			rh_vsource = h5py.VirtualSource(hdf5_file_path, 'rh_data', shape=f['rh_data'].shape)
			hdf5_layout[image_data_dict['rh_data_inds'],:] = rh_vsource

			print('Added RH data to HDF5')



		#Add nifti data
		if has_nifti:
			#if inds_counted > 0:
			#	nifti_data_inds = np.arange(inds_counted, inds_counted + f['nifti_data'].shape[0], 1, dtype=int)
			#	data[inds_counted:(inds_counted + f['nifti_data'].shape[0]),:] = f['nifti_data']
			#else:
			#	nifti_data_inds = np.arange(0, f['nifti_data'].shape[0], 1, dtype=int)
			#	data = f['nifti_data']

			f['nifti_data_inds'] = np.arange(inds_counted, inds_counted + f['nifti_data'].shape[0], 1, dtype=int)
			#data[nifti_data_inds,:] = f['nifti_data']
			print('Made Nifti inds')

			nifti_vsource = h5py.VirtualSource(hdf5_file_path, 'nifti_data', shape=f['nifti_data'].shape)
			hdf5_layout[f['nifti_data_inds'],:] = nifti_vsource

			print('Added Nifti data to HDF5')

		#Add all the different datasets to a new
		#virtual dataset
		f.create_virtual_dataset('data', hdf5_layout)
		if has_lh_gifti:
			if has_lh_gifti_parcellation:
				f['ids'].attrs['lh_ids'] = lh_gifti_ids
			else:
				f['/ids/lh_ids'] = lh_gifti_ids
		if has_rh_gifti:
			if has_rh_gifti_parcellation:
				f['ids'].attrs['rh_ids'] = rh_gifti_ids
			else:
				f['/ids/rh_ids'] = rh_gifti_ids
		if has_nifti:
			if has_nifti_parcellation:
				f['ids'].attrs['nifti_ids'] = nifti_ids
			else:
				f['/ids/nifti_ids'] = np.asarray(nifti_ids)

		data = f['data']


		#Normalize data if necessary
		if normalize_within_dataset == True:
			if data.shape[1] > 1:

				data_means = np.mean(data,axis=1)
				data = data/data_means[:,np.newaxis]*1000
				f['data_means'] = data_means

				print('Finished Normalizing Data Data')



		_dict_to_hdf5_attrs(f['data'], metadata_dict)
		_dict_to_hdf5_attrs(f['data'], file_path_dictionary)
		f.flush()
		print('Finished Flushing')


	return


def _dict_to_hdf5_attrs(hdf5_dataset_object, dictionary, base_path = ''):
	"""Function adds dictionary to hdf5 file

	Function takes a HDF5 dataset, and adds dictionary
	key/value pairs as new datasets under the desired base
	path (so you can have dictionary be put into sub groups
	instead of main group, otherwise leave base_path empty).

	Parameters
	----------

	hdf5_file_object : hdf5
		hdf5 file loaded from h5py
	dictionary : dict
		dictionary to be added to hdf5 file as attributes
	base_path : str, optional
		optional if you want to have dictionary nested within
		an hdf5 group

	"""

	for key, value in dictionary.items():

		hdf5_dataset_object.attrs[os.path.join(base_path,key)] = value

	return

def _dict_to_hdf5_subdatasets(hdf5_object, dictionary, base_path = ''):
	"""Function adds dictionary to hdf5 file

	Function takes a HDF5 dataset, and adds dictionary
	key/value pairs as new datasets under the desired base
	path (so you can have dictionary be put into sub groups
	instead of main group, otherwise leave base_path empty).

	Parameters
	----------

	hdf5_file_object : hdf5
		hdf5 file loaded from h5py
	dictionary : dict
		dictionary to be added to hdf5 file as attributes
	base_path : str, optional
		optional if you want to have dictionary nested within
		an hdf5 group

	"""

	for key, value in dictionary.items():

		hdf5_object.create_dataset(os.path.join(base_path, key), data = value)

	return
